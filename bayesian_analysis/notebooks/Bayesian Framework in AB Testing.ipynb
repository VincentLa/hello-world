{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Framework in A/B Testing\n",
    "In this notebook we will discuss using a bayesian framework in A/B testing. We will also compare and contrast with the more often used frequentist framework in A/B testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is A/B Testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tech world's nomenclature for \"randomized control trial\". A quick example which we will use throughout the rest of this notebook is Clover's medication adherence intervention (one of our first experiments truly run in the field). **In this notebook we will discuss a very simplified view of this experiment with fake results**. Don't focus too much on the actual numbers, just the concept.\n",
    "\n",
    "1. The \"A\" group is traditionally considered as the control. In this example, suppose we put many members into the \"A\" group and do not carry out any intervention.\n",
    "2. The \"B\" group is traditionally the intervention group. In this example, suppose the intervention is a phone call reminding the member to pick up their medication.\n",
    "\n",
    "The idea is we randomly split a subset of members into A and B group and test the effect of our intervention (a phone call) on medication fill rates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Frequentist Perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequentist framework in evaluating A/B tests generally\n",
    "1. does not take into account prior information; comparisons between the A and B group rely solely on the data from the experiment with no incorporation of prior beliefs.\n",
    "2. does not view results as probability distribution. In other words, in this example, we want to compare the fill rate between the two groups. A frequentist would generally view the fill rate as unknown, but fixed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally an A/B test done with a frequentist perspective would look like the following:\n",
    "1. Perform some kind of power calculation to calculate the required sample size in advance. The power calculation will depend on $\\alpha$, $\\beta$, and the effect size you want to detect. \n",
    "2. Randomly split into A/B groups and apply intervention to B group.\n",
    "3. Launch the test and let it run until the required sample size is reached (without peaking at the data, or at least not acting on peaking at the data).\n",
    "4. Analyze results! In the frequentist world, this normally involves calculating some p-value comparing the fill rates between the two groups. The general interpretation of said p-value would be \"assuming that there is actually no difference between the A and B group, what is the probability we would see a difference in fill rate at least as extreme as we observed\".\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bayesian Perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main differentiating factor between the bayesian perspective vs the frequentist perspective is that in a bayesian world, we view results as a probability distribution of the outcome of interest, as opposed to a fixed number. In addition, a bayesian framework gives us a natural way to incorporate prior distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example of using an A/B test to evaluate the effect of a call campaign on medication fill rates, in a strict frequentist view, the framework would be that we're estimating the \"true\" fill rate in each group with a fixed number. In a bayesian view, we estimate the fill rate as a probability distribution given some prior (that we need to define) and updating that prior with the experiment results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How an A/B test is conducted from a Bayesian perspective is still largely the same. The main difference will be in how we actually analyze the results. In particular, \"p-value\" is not a term that exists in Bayesian world, so how do we compare groups A and B? What is the result that is returned and how do we interpret it? These are best dealt with using an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that there are 300 members that would qualify for a call campaign on medication fill rates. We're only going to be running our test on 150 of these members, with 150 remaining as control. Group A will receive whatever normal interventions from Clover. Group B will as well, but now in addition, members in group B also receive a phone call to remind them to pick up their medication.\n",
    "\n",
    "Next we need to figure out what prior probability we are going to use. This is one of the tougher parts about using a Bayesian framework (more on that below). Let's say we have a reasonable expectation that the probability of the member filling their prescription should be around 30%. To make things simple we'll use the same prior for A and B. We'll also choose a pretty weak version of our prior for illustrative purposes -- we'll settle on Beta(3,7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from clover.dsa.tools import beta_binomial as bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distribution_values = bb.beta_values(alpha=3, beta=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(distribution_values[0], distribution_values[1])\n",
    "plt.xlabel('Probability of Filling')\n",
    "plt.title('PDF of Prior distribution of Member Filling Medication')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's say we actually run the experiment and get these results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   \t|Fill   \t|Not Fill   \t\n",
    "|---\t|---\t|---\t|---\t|---\t|\n",
    "|Group A   \t|36   \t|114\t\n",
    "|Group B   \t|50  \t|100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a different notebook, we discuss the update rules using the beta-binomial distribution. We won't go through that again here, but it turns out that using the experiment results, we update our prior to obtain the posterior distributions for the fill rate of Group A vs Group B as\n",
    "$$Beta(3 + 36, 7 + 114)$$\n",
    "$$Beta(3 + 50, 7 + 100)$$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting these distributions side by side we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_sa, beta_sa, alpha_sb, beta_sb = 3 + 36, 7 + 114, 3 + 50, 7 + 100\n",
    "distribution_values_a = bb.beta_values(alpha=alpha_sa, beta=beta_sa)\n",
    "distribution_values_b = bb.beta_values(alpha=alpha_sb, beta=beta_sb)\n",
    "plt.scatter(distribution_values_a[0], distribution_values_a[1], color='red')\n",
    "plt.scatter(distribution_values_b[0], distribution_values_b[1], color='blue')\n",
    "plt.xlabel('Probability of Filling')\n",
    "plt.title('PDF of Posterior distribution of Member Filling Medication')\n",
    "plt.xlim(0.1, 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the distributions plotted above, it certainly seems like B group (blue line) performed better! But notice we actually have the full distributions of the estimate of the fill rate in group A and B. In particular, with those full distributions, we are actually empowered to do a lot of things. In particular, if we wanted to calculate the probability group B is better than A, we can do so! One intuitive way is to simulate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np    \n",
    "sa_samples = np.random.beta(alpha_sa, beta_sa, 100000)\n",
    "sb_samples = np.random.beta(alpha_sb, beta_sb, 100000)\n",
    "\n",
    "# Simulation Results\n",
    "np.sum(sb_samples > sa_samples) / len(sa_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that this implementation is also already defined in our `data_science` repo.) By default, this simply simulates 100,000 random samples from the beta distribution defined by the parameters returned above and compares the proportion of observations from sample B that are greater than sample A. In this example, sample B is about 96% \"better\" than sample A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But really why is this better than just evaluating the p-value in the frequentist world?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Significance can at most tell us \"these two things are not likely the same\" (this is what rejecting the Null Hypothesis is saying). That's not really a great answer for an A/B Test. We're running this test because we want to improve fill rates. Results that say \"Group B where medication adherence calls are made will probably do better\" are okay, but don't you really want to know how much better? Frequentist world tells us significance, but what we're really after is **magnitude!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting the Distribution\n",
    "plt.hist(sb_samples / sa_samples)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this histogram we can see that our most likely cases is that B is about a 40% improvement over A, but we can see an entire range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the CDF which is also super informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting the CDF \n",
    "import statsmodels.api as sm\n",
    "ecdf = sm.distributions.ECDF(sb_samples / sa_samples)\n",
    "\n",
    "x = np.linspace(min(sb_samples / sa_samples), max(sb_samples / sa_samples), 1000)\n",
    "y = ecdf(x)\n",
    "plt.step(x, y)\n",
    "plt.ylabel(\"Cumulative probabitlity\")\n",
    "plt.xlabel(\"sb_samples / sa_samples\")\n",
    "plt.title(\"CDF of sb_samples / sa_samples\")\n",
    "\n",
    "plt.axhline(0.5, color='red', linestyle='dashed', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the red line is drawn at the median (in this case the median improvement of B over A is about 40%).\n",
    "\n",
    "Now we can see that there is really just a small, small chance that A is better, but even if it is better it's not going to be better by much. We can also see that there's about a 25% chance that Variant B is a 50% or more improvement over A, and even a reasonable (but quite small) chance it could be more than double the fill rate! Now in choosing B over A we can actually reason about our risk, for example, \"The chance that B is 20% worse is roughly the same that it's 100% better.\" In short, with the full probability distribution, we can say way more nuanced things other than \"we reject the null hypothesis\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cons of Bayesian Analysis\n",
    "This is not to say that Bayesian framework is universally dominant over the Frequentist framework along every dimension (otherwise why would any A/B test evaluator use a frequentist approach ever?). Some frequently quoted cons of Bayesian analysis are:\n",
    "1. **Difficulty of coming up with and defending a prior**: this is the most frequent criticism of bayesian analysis. The most common way to deal with this is to do sensitivity testing on your prior and see if your results vastly change if you alter the prior reasonably. If not, that's generally a good sign. If results are very sensitive, there's probably more work you need to do.\n",
    "2. **Computationally intensive**: bayesian analysis also tends to be more computationally intensive which is probably one of the contributing reasons why they were not as frequently used in the past. Modern technology, in most cases relevant to Clover, probably has taken care of this for us.\n",
    "3. **Hard to come up with an analytical solution**: goes along with number 2, but certain types of bayesian frameworks don't necessarily \"converge\" nicely. That being said one of the reasons for choosing the beta-binomial framework is the nice mathematical/convergence properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful table (?) taken from https://www.dynamicyield.com/2016/09/bayesian-testing/\n",
    "\n",
    "|   \t|Frequentist   \t|Bayesian A/B Testing   \t\n",
    "|---\t|---\t|---\t|---\t|---\t|\n",
    "|Knowledge of Baseline Performance   \t|Required   \t|Not Required   \t\n",
    "|Intuitiveness   \t|Less, as p-value is a convoluted term   \t|More, as we directly calculate the probability of A being better than B   \n",
    "|Sample Size   \t|Pre-defined   \t|No need to pre-define   \t\n",
    "|Peeking at the data while the test runs    | Not allowed    | Allowed (with caution)\n",
    "|Quick to make decisions    | Less, as it has more restrictive assumptions on distributions    |More, as it has less restrictive assumptions\n",
    "|Representing uncertainty    |Confidence Interval (somewhat convoluted and frequently misunderstood)    |Highest Posterior Density Region -- highly intuitive interpretation\n",
    "|Declaring a winner    |When sample size is reached and p-value is below a certain threshold    |When either \"probability to be best\" goes above a threshold or the expected loss is below a threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Related -- Difference between credible interval (bayesian) and confidence interval (frequentist): https://stats.stackexchange.com/questions/2272/whats-the-difference-between-a-confidence-interval-and-a-credible-interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some common followup questions\n",
    "1. Do you need \"less n\" using a bayesian approach?\n",
    "2. Can you \"peak early\" using a bayesian approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
