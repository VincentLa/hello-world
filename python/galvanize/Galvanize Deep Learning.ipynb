{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Classification Modeling using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Galvanize: 2017-04-29\n",
    "\n",
    "Slides http://lukas.show/, https://s3.amazonaws.com/ai-learn-l2k/ML_Course.pdf\n",
    "\n",
    "GitHub: https://github.com/lukas/scikit-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/VincentLa/venv/india-images/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import keras\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = tweets.is_there_an_emotion_directed_at_a_brand_or_product\n",
    "text = tweets.tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    .@wesley83 I have a 3G iPhone. After 3 hrs twe...\n",
       "1    @jessedee Know about @fludapp ? Awesome iPad/i...\n",
       "2    @swonderlin Can not wait for #iPad 2 also. The...\n",
       "3    @sxsw I hope this year's festival isn't as cra...\n",
       "4    @sxtxstate great stuff on Fri #SXSW: Marissa M...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First need to remove null texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fixed_text = text[pd.notnull(text)]\n",
    "fixed_target = target[pd.notnull(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect=CountVectorizer()\n",
    "count_vect.fit(fixed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count_vect.vocabulary is a dictionary\n",
    "count_vect.fit(fixed_text) is a nxm matrix where n is the number of tweets and m is the number of total words. An entry (i, j) in the matrix is how many times word j occurs in tweet i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8562\n",
      "3108\n"
     ]
    }
   ],
   "source": [
    "# What is the column of the word the\n",
    "print(count_vect.vocabulary_.get(u'the'))\n",
    "print(count_vect.vocabulary_.get(u'eye'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9706"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform the text of the tweets into a matrix\n",
    "counts = count_vect.transform(fixed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    .@wesley83 I have a 3G iPhone. After 3 hrs twe...\n",
      "1    @jessedee Know about @fludapp ? Awesome iPad/i...\n",
      "Name: tweet_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(fixed_text[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 168)\t1\n",
      "  (0, 430)\t1\n",
      "  (0, 774)\t2\n",
      "  (0, 2291)\t1\n",
      "  (0, 3981)\t1\n",
      "  (0, 4210)\t1\n",
      "  (0, 4573)\t1\n",
      "  (0, 4610)\t1\n",
      "  (0, 5766)\t1\n",
      "  (0, 6478)\t1\n",
      "  (0, 7232)\t1\n",
      "  (0, 8076)\t1\n",
      "  (0, 8323)\t1\n",
      "  (0, 8702)\t1\n",
      "  (0, 8920)\t1\n",
      "  (0, 9062)\t1\n",
      "  (0, 9303)\t1\n",
      "  (0, 9373)\t1\n"
     ]
    }
   ],
   "source": [
    "# See counts for the first tweet\n",
    "print(counts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4573)\t1\n"
     ]
    }
   ],
   "source": [
    "print(count_vect.transform(['iPhone']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a naive bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No emotion toward brand or product']\n",
      "['Negative emotion']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(counts, fixed_target)\n",
    "\n",
    "print(nb.predict(count_vect.transform([\"iphone!!!\"])))\n",
    "print(nb.predict(count_vect.transform([\"iphone does not suck\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.795094588649\n"
     ]
    }
   ],
   "source": [
    "predictions = nb.predict(counts)\n",
    "correct = sum(predictions == fixed_target)\n",
    "incorrect = sum(predictions != fixed_target)\n",
    "acc = correct/(len(fixed_target))\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacky way to do a hold-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb.fit(counts[0:6000], fixed_target[0:6000])\n",
    "predictions = nb.predict(counts[6000:9092])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.225802903652\n"
     ]
    }
   ],
   "source": [
    "correct = sum(predictions == fixed_target[6000:9092])\n",
    "incorrect = sum(predictions != fixed_target[6000:9092])\n",
    "acc = correct/(len(fixed_target))\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Deep Learning                                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good resource: http://neuralnetworksanddeeplearning.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.63516484  0.65274725  0.64175824  0.63626374  0.62087912  0.61428571\n",
      "  0.62706271  0.58305831  0.53031974  0.45644983]\n",
      "0.599798948321\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "perceptron = Perceptron()\n",
    "\n",
    "from sklearn import cross_validation\n",
    "\n",
    "scores = cross_validation.cross_val_score(perceptron, counts, fixed_target, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras for image learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor is essentially matrix of arbitrary dimensions. It is made up of n x m dimension arrays?\n",
    "Question: How do we prepare the data set?\n",
    "\n",
    "Keras documentation: https://keras.io/models/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "            .....XX.XXX.    \n",
      "        ...XXXXXXXXXXXX.    \n",
      "       .XXXXXXXXXX.....     \n",
      "       .XXXXXXXXXX          \n",
      "        .X.XXX. .X          \n",
      "         ..XX.              \n",
      "           XXX.             \n",
      "           .XX.             \n",
      "            .XXX..          \n",
      "             .XXX..         \n",
      "              .XXXX.        \n",
      "               ..XXX        \n",
      "                 XXX.       \n",
      "              .XXXXX.       \n",
      "            .XXXXXXX        \n",
      "          ..XXXXXX.         \n",
      "        ..XXXXXX..          \n",
      "      .XXXXXXX..            \n",
      "    .XXXXXXXX.              \n",
      "    XXXXXXX.                \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "\n",
      "Label:  5\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "digit = X_train[0]\n",
    "print(digit.shape)\n",
    "str = \"\"\n",
    "# Doing some hacky drawing\n",
    "for i in range(digit.shape[0]):\n",
    "    for j in range(digit.shape[1]):\n",
    "        if digit[i][j] == 0:\n",
    "            str += \" \"\n",
    "        elif digit[i][j] < 128:\n",
    "            str += \".\"\n",
    "        else:\n",
    "            str += \"X\"\n",
    "    str += \"\\n\"\n",
    "\n",
    "print(str)\n",
    "print(\"Label: \", y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(digit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.88661339  0.87922705  0.84685886  0.87716667  0.87366667  0.8708118\n",
      "  0.87747958  0.84664111  0.73169918  0.89292862]\n",
      "0.85830929207\n"
     ]
    }
   ],
   "source": [
    "# Now see how well the perceptron is actually doing\n",
    "X_train = [x.flatten() for x in X_train]\n",
    "perceptron = Perceptron()\n",
    "scores = cross_validation.cross_val_score(perceptron, X_train, y_train, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: What is tensorflow\n",
    "\n",
    "A Tensor object is a symbolic handle to the result of an operation, but does not actually hold the values of the operation's output. Instead, TensorFlow encourages users to build up complicated expressions (such as entire neural networks and its gradients) as a dataflow graph. You then offload the computation of the entire dataflow graph (or a subgraph of it) to a TensorFlow tf.Session, which is able to execute the whole computation much more efficiently than executing the operations one-by-one.\n",
    "\n",
    "https://www.tensorflow.org/programmers_guide/faq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        x.initializer.run()\n",
    "        y.initializer.run()\n",
    "        result = f.eval()\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_train is a n x 1 array that are the actual labels of the digit. We're one-hot encoding it to make it a n x 10 array where each column is an indicator for whether the value is equal to the digit that the column represents.\n",
    "\n",
    "X_train is a 3 dimensional numpy matrix that represents the actual image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "img_width = X_train.shape[1]\n",
    "img_height = X_train.shape[2]\n",
    "\n",
    "# Normalizing the data set helps with Keras's implementation. The 255 I think is coming from the valid RGB values.\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.\n",
    "X_test /= 255.\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s - loss: 0.4692 - acc: 0.8765     \n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s - loss: 0.3042 - acc: 0.9148     \n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s - loss: 0.2834 - acc: 0.9206     \n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s - loss: 0.2730 - acc: 0.9238     \n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.2659 - acc: 0.9261     \n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.2617 - acc: 0.9271     \n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s - loss: 0.2587 - acc: 0.9281     \n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s - loss: 0.2551 - acc: 0.9289     \n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s - loss: 0.2523 - acc: 0.9302     \n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s - loss: 0.2504 - acc: 0.9305     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11a552ef0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model=Sequential()\n",
    "model.add(Flatten(input_shape=(img_width,img_height)))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.2488 - acc: 0.9305 - val_loss: 0.2690 - val_acc: 0.9270\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s - loss: 0.2480 - acc: 0.9310 - val_loss: 0.2646 - val_acc: 0.9273\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.2458 - acc: 0.9327 - val_loss: 0.2681 - val_acc: 0.9259\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.2444 - acc: 0.9326 - val_loss: 0.2685 - val_acc: 0.9264\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.2433 - acc: 0.9330 - val_loss: 0.2711 - val_acc: 0.9256\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.2427 - acc: 0.9332 - val_loss: 0.2683 - val_acc: 0.9260\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.2412 - acc: 0.9329 - val_loss: 0.2681 - val_acc: 0.9291\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.2408 - acc: 0.9338 - val_loss: 0.2704 - val_acc: 0.9277\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.2402 - acc: 0.9340 - val_loss: 0.2663 - val_acc: 0.9262\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s - loss: 0.2389 - acc: 0.9339 - val_loss: 0.2701 - val_acc: 0.9271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11a7f2940>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 1s - loss: 0.6027 - acc: 0.8370 - val_loss: 0.2961 - val_acc: 0.9208\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.2725 - acc: 0.9233 - val_loss: 0.2308 - val_acc: 0.9364\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.2248 - acc: 0.9361 - val_loss: 0.2018 - val_acc: 0.9428\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.1949 - acc: 0.9447 - val_loss: 0.1819 - val_acc: 0.9497\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.1750 - acc: 0.9497 - val_loss: 0.1663 - val_acc: 0.9537\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.1587 - acc: 0.9537 - val_loss: 0.1521 - val_acc: 0.9561\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.1465 - acc: 0.9571 - val_loss: 0.1514 - val_acc: 0.9563\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.1364 - acc: 0.9605 - val_loss: 0.1384 - val_acc: 0.9609\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.1275 - acc: 0.9629 - val_loss: 0.1327 - val_acc: 0.9610\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 0s - loss: 0.1200 - acc: 0.9656 - val_loss: 0.1275 - val_acc: 0.9641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11dd15da0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model=Sequential()\n",
    "model.add(Flatten(input_shape=(img_width,img_height)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
