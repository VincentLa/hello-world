{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 120 Interview Questions Answers\n",
    "Note, there are a lot of user-driven answers as well. See https://github.com/JifuZhao/120-DS-Interview-Questions/blob/master/predictive-modeling.md for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "(Given a Dataset) Analyze this dataset and give me a model that can predict this response variable.\n",
    "\n",
    "Answer:\n",
    "Initial Exploratory Analysis\n",
    "1. Check the shape of the data. How long is it, how wide is it. If very wide but not long we might run into variance issues\n",
    "2. Check for class imbalance\n",
    "3. Check for missing data. What does missing mean? Is it missing at random? Is there selection bias in the way the data is missing?\n",
    "\n",
    "Feature Extraction\n",
    "1. First, is there subject matter expertise on how to pick features. While there are feature selection methods to allow the algorithm to pick which features are most important, it's good to come in with a priori knowledge around which features to include. Otherwise we could just be throwing in junk and have bias/variance tradeoff\n",
    "2. Think of ways to hierarchically group features, if you have too many features that could lead to high variance \n",
    "3. One-hot encode categorical features if necessary\n",
    "\n",
    "Model Fitting\n",
    "1. Again we have to think about bias/variance tradeoff. I always like to start with two models\n",
    "2. Logistic Regression: A simple model that is highly explainable; especially in healthcare setting, having coefficients and being able to explain a model is useful. However, tends to be high bias, but lower variance, since you're imposing a linear classifier\n",
    "3. Random Forest: Takes into account non-linearities and interaction effects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "**Need to read more on this**\n",
    "\n",
    "What could be some issues if the distribution of the test data is significantly different than the distribution of the training data?\n",
    "\n",
    "Answer:\n",
    "\n",
    "In this case, there will definitely be issues on model performance since the model is trained on the training set will fit to the training set, and if the distribution is different in the test set, the model will not perform as well. In production this could very well happen if populations shift. In the healthcare setting, imagine training your model using one hospital data, but then you apply that model to a completely different setting; your model may not perform as well.\n",
    "\n",
    "Very commonly, we call this \"dataset shift\". There are three aspects to dataset shift\n",
    "- Covariate Shift\n",
    "- Prior Probability Shift\n",
    "- Concept Shift\n",
    "\n",
    "See https://medium.com/capital-one-tech/domain-adaptation-5955edf0277b for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "What are some ways I can make my model more robust to outliers?\n",
    "\n",
    "Answer:\n",
    "Regularization, bagging, using Mean Absolute Deviation as opposed to Mean Squared Error in a regression problem\n",
    "\n",
    "Tree based models tend to also do better with outliers because you're typically just splitting into regions. So for tree based models the scale of the features don't matter as much just the relative ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "What are some differences you would expect in a model that minimizes squared error, versus a model that minimizes absolute error? In which cases would each error metric be appropriate?\n",
    "\n",
    "Answer: Absolute error is more robust to outliers but harder to computationally fit. If we do actually care about consequences of large errors than we should use MSE since MSE will be more likely to minimize that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "**Need to read more about this especially for multiclass problems**\n",
    "What error metric would you use to evaluate how good a binary classifier is? What if the classes are imbalanced? What if there are more than 2 groups?\n",
    "\n",
    "Answer: Accuracy, AUPR, AUROC\n",
    "\n",
    "AUPR: Recall vs Precision\n",
    "AUROC: Recall vs False Positive Rate\n",
    "\n",
    "On more than 2 groups, the idea is similar where you can calculate recall/precision on each of the classes. To compute an F1 score, you can do something similar where you weight/average all the scores together.\n",
    "\n",
    "See https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1 for an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "**Need to look into this more. I have a high level response, but it's good to go over each of the standard models and know how they work**\n",
    "\n",
    "What are various ways to predict a binary response variable? Can you compare two of them and tell me when one would be more appropriate? Whatâ€™s the difference between these? (SVM, Logistic Regression, Naive Bayes, Decision Tree, etc.)\n",
    "\n",
    "Answer\n",
    "\n",
    "High level there are parametrics and non-parametric models and ultimately what you choose is based on bias/variance tradeoff.\n",
    "\n",
    "Logistic, linear classifier, fits based on Maximum Likelihood method where you're trying to fit against the log odds. \n",
    "Random Forest: tree based classifier, similar to decision tree in that you're trying to minimize entropy and maximize information gain but to reduce risk of overfitting you only consider a subset of all features at each split. (**Need to look into this more**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "What is regularization and where might it be helpful? What is an example of using regularization in a model?\n",
    "\n",
    "Answer:\n",
    "At a high level, regularization is basically adding a penalty term for more complex models. The reason why you might do this is going back to fundamental bias-variance tradeoff. If you have a very complex model, you might have low bias but have high variance so you will overfit the training data. Regularization will add a penalty term for this.\n",
    "\n",
    "Typically, there is a concept of L1 vs L2 regularization. In a regression context, L1 is what a Lasso regression uses whereas L2 is what a Ridge regression uses. At a high level both will add a penalty term on the coefficients. The main difference in outcome is that the Lasso regression can set some coefficients to 0 whereas the Ridge regression will not. The mechanisms are also different\n",
    "\n",
    "- L1 adds a penalty term by adding a penalty for the sum of the absolute value of coefficients\n",
    "- L2 adds a penalty term by adding a penalty for the sum of squares of the coefficients\n",
    "\n",
    "https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization-How-does-it-solve-the-problem-of-overfitting-Which-regularizer-to-use-and-when is pretty good at explaining this as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "Why might it be preferable to include fewer predictors over many?\n",
    "\n",
    "Answer:\n",
    "Bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "What are the different types of joins? What are the differences between them?\n",
    "\n",
    "1. Inner\n",
    "2. Left\n",
    "3. Right\n",
    "4. Full Outer\n",
    "5. Cross Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "Why might a join on a subquery be slow? How might you speed it up?\n",
    "\n",
    "Answer:\n",
    "Lack of indices, query planner on a subquery is forced by user. Tradeoff is traditionally readability vs performance. One way to speed it up is to materialize the actual view as a table.\n",
    "\n",
    "Stackoverflow answer: https://stackoverflow.com/questions/31724903/why-might-a-join-on-a-subquery-be-slow-what-could-be-done-to-make-it-faster-s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "Describe the difference between primary keys and foreign keys in a SQL database.\n",
    "\n",
    "Primary key uniquely identifies the column in a SQL table and often adds an index. Foreign key is a reference from another table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12\n",
    "Given a `COURSES` table with columns `course_id` and `course_name`, a `FACULTY` table with columns `faculty_id` and `faculty_name`, and a `COURSE_FACULTY` table with columns `faculty_id` and `course_id`, how would you return a list of faculty who teach a course given the name of a course?\n",
    "\n",
    "```\n",
    "select\n",
    "  courses.course_name,\n",
    "  faculty.faculty_id,\n",
    "  faculty.faculty_name\n",
    "from courses\n",
    "  join course_faculty\n",
    "    on courses.course_id = course_faculty.course_id\n",
    "  join faculty\n",
    "    on course_faculty.faculty_id = faculty.faculty_id\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13\n",
    "Given a `IMPRESSIONS` table with `ad_id`, `click` (an indicator that the ad was clicked), and `date`, write a SQL query that will tell me the click-through-rate of each ad by month.\n",
    "\n",
    "```\n",
    "select\n",
    "  date_trunc('month', date) as month,\n",
    "  sum(click) as total_clicks,\n",
    "  count(*) as total_adds,\n",
    "  sum(click) / count(*) as ctr_per_month\n",
    "from impressions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14\n",
    "Write a query that returns the name of each department and a count of the number of employees in each:\n",
    "`EMPLOYEES` containing: `Emp_ID` (Primary key) and `Emp_Name`\n",
    "\n",
    "`EMPLOYEE_DEPT` containing: `Emp_ID` (Foreign key) and `Dept_ID` (Foreign key)\n",
    "\n",
    "`DEPTS` containing: `Dept_ID` (Primary key) and `Dept_Name`\n",
    "\n",
    "```\n",
    "select\n",
    "  depts.dept_name,\n",
    "  count(*) as number_employees\n",
    "from employee_dept\n",
    "  join depts\n",
    "    on employee_dept.dept_id = depts.dept_id\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
